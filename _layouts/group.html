---
layout: main
css: ../stylesheets/stylesheet.css
---
<p>Expliquer comment se logger aux machines amazon</p>

<h1>Logstash</h1>
<p>Logstash est un pipe permettant de collecter, parser, et stocker des logs à l'aide d'entrées, de filtres et de sorties (input, filter, output). La phase de parsing permet d'ajouter de la sémantique à notre événement, en ajoutant, modifiant ou supprimant des champs, des tags, des types, etc...</p>

<p>Dans cette première partie de l'atelier, nous allons donc découvrir logstash et le configurer pour structurer nos logs afin qu'ils soient facilement exploitables par la suite.</p>

<h2>Découverte de logstash</h2>
<p>Copier le jar logstash disponible dans le répertoire <code>~/tools</code> dans le répertoire <code>~/workshop</code></p>
<p>Dans le répertoire <code>~/workshop</code>, créer un fichier de configuration logstash nommé <code>logstash-logback.conf</code></p>

{% highlight kconfig %}
input {
  stdin { } 
}
 
output {
  stdout { debug => true }
}
{% endhighlight %}

<p>Vous pouvez maintenant exécuter logstash grâce à la commande suivante:</p>
{% highlight bash %}
$ java -jar logstash-1.2.1-flatjar.jar agent -f logstash-logback.conf
{% endhighlight %}

<p>Logstash est prêt à interpréter ce qu'il recevera sur l'entrée standard. Pour un premier test, passez lui la ligne suivante en entrée:</p>

{% highlight bash %}02-10-2013 14:26:27.724 [pool-10-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=9205,ip=217.109.49.180,cat=TSHIRT{% endhighlight %}

<p>Vous devez normalement voir un message de la forme suivante s'afficher:</p>
{% highlight bash %}
{
    "message" => "02-10-2013 14:26:27.724 [pool-10-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=9205,ip=217.109.49.180,cat=TSHIRT",
    "@timestamp" => "2013-10-25T07:50:26.232Z",
    "@version" => "1",
    "host" => "lucid64"
}
{% endhighlight %}

<p>On constate que le timestamp enregistré n'est pas du tout lié à celui de la ligne de log. En fait, logstash n'a pas du tout interprété le message. Pour faire celà, il va falloir configurer un filtre pour logstash.</p>

<h2>Ajouter de la sémantique</h2>

<p>Pour configurer logstash afin qu'il puisse interpréter les données qu'il va recevoir en entrée, nous allons configurer plusieurs filtres.</p>

<h3>Filtre Grok</h3>

<p>Le filtre Grok met à votre disposition plusieurs patterns pour parser les lignes de logs.</p>

<p>Ressources:
	<ul>
		<li><a target="_blank" href="http://logstash.net/docs/1.2.1/filters/grok">Documentation du filtre Grok</a></li>
		<li><a target="_blank" href="https://github.com/logstash/logstash/blob/master/patterns/grok-patterns">Les patterns grok pré-définis</a></li>
		<li><a target="_blank" href="http://grokdebug.herokuapp.com/">Debugger</a></li>
	</ul>
</p>

<p>Dans un premier temps, nous voulons juste parser le niveau de log. Pour celà, le pattern <code>LOGLEVEL</code> va nous être utile.</p>

{% highlight bash %}
filter {
   grok {
      match => ["message","%{LOGLEVEL:log_level}"]
   }
}
{% endhighlight %}

<p>Vous pouvez relancer logstash et repasser la ligne de log:</p>

{% highlight bash %}02-10-2013 14:26:27.724 [pool-10-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=9205,ip=217.109.49.180,cat=TSHIRT{% endhighlight %}

<p>Cette fois, on remarque qu'un élément a été analysé, l'élément <code>log_level</code></p>

{% highlight bash %}
{
    "message" => "02-10-2013 14:26:27.724 [pool-10-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=9205,ip=217.109.49.180,cat=TSHIRT",
    "@timestamp" => "2013-10-27T08:46:51.132Z",
    "@version" => "1",
    "host" => "lucid64",
    "log_level" => "INFO"
}
{% endhighlight %}

<p>Nous allons enrichir notre filtre pour parser le reste des lignes:</p>

{% highlight bash %}
filter {
   grok {
      match => ["message","(?<log_date>%{MONTHDAY}-%{MONTHNUM}-%{YEAR} %{HOUR}:%{MINUTE}:%{SECOND}.[0-9]{3}) \[%{NOTSPACE:thread}\] %{LOGLEVEL:log_level} %{NOTSPACE:classname} - %{GREEDYDATA:msg}"]
   }
}
{% endhighlight %}

<p>De nouveau, nous pouvons relancer logstash et lui passer notre ligne de log, cette fois, nous allons avoir un résultat de la forme suivante:</p>

{% highlight bash %}
{
    "message" => "02-10-2013 14:26:27.724 [pool-10-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=9205,ip=217.109.49.180,cat=TSHIRT",
    "@timestamp" => "2013-10-27T10:30:32.242Z",
    "@version" => "1",
    "host" => "lucid64",
    "log_date" => "02-10-2013 14:26:27.724",
    "thread" => "pool-10-thread-1",
    "log_level" => "INFO",
    "classname" => "com.github.vspiewak.loggenerator.SearchRequest",
    "msg" => "id=9205,ip=217.109.49.180,cat=TSHIRT"
}
{% endhighlight %}

<p>Pour alléger la configuration, nous allons extraire l'expression régulière de la date et la définir en tant que pattern:
	<ul>
		<li>créez un dossier "patterns" dans le répertoire courant</li>
		<li>Créez un fichier "logback" dans le dossier "patterns" contenant:</li>
{% highlight bash %}
LOG_DATE %{MONTHDAY}-%{MONTHNUM}-%{YEAR} %{HOUR}:%{MINUTE}:%{SECOND}.[0-9]{3}
{% endhighlight %}		
	</ul>
Indiquez ensuite à Grok le dossier contenant vos fichiers de patterns via l'attribut "patterns_dir".
</p>

<p>La configuration Losgstash devient:
{% highlight bash %}
filter { 
   grok {
      patterns_dir => "./patterns"
      match => [ "message", "%{LOG_DATE:log_date} \[%{NOTSPACE:thread}\] %{LOGLEVEL:log_level} %{NOTSPACE:classname} - %{GREEDYDATA:msg}"]
   }
}
{% endhighlight %}      

</p>

<p>À ce niveau, nous constatons que l'intégralité du message a été parsé et est maintenant interprété. Par contre, même si nous avons correctement interprété la date du log, le champ <code>@timestamp</code> contient toujours la date de lecture par logstash. Il serait plus intéressant de mettre dans ce champ la date de log. Pour celà, il va falloir utiliser un autre type de filtre, le filtre date.</p>

<h3>Filtre date</h3>

<p>Ressources:
    <ul>
        <li><a target="_blank" href="http://logstash.net/docs/1.2.1/filters/date">Documentation du filtre date</a></li>
    </ul>
</p>

<p>Le filtre date est l'un des filtre les plus important. Il permet en effet de parser une date et de l'utiliser pour le champ <code>@timestamp</code>. Rajoutez le filtre dans le fichier de configuration:
{% highlight bash %}
filter {
    [...]
    date {
        match => ["log_date","dd-MM-YYYY HH:mm:ss.SSS"]
    }
}
{% endhighlight %}
</p>

<p>En redonnant notre ligne de log en entrée, nous récupérons un retour de la forme suivante:
{% highlight bash %}
{
    "message" => "02-10-2013 14:26:27.724 [pool-10-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=9205,ip=217.109.49.180,cat=TSHIRT",
    "@timestamp" => "2013-10-02T12:26:27.724Z",
    "@version" => "1",
    "host" => "lucid64",
    "log_date" => "02-10-2013 14:26:27.724",
    "thread" => "pool-10-thread-1",
    "log_level" => "INFO",
    "classname" => "com.github.vspiewak.loggenerator.SearchRequest",
    "msg" => "id=9205,ip=217.109.49.180,cat=TSHIRT"
}
{% endhighlight %}
</p>

<p>PARLER DE LA TIMEZONE ??</p>

<h3>Filtre kv</h3>

<p>Le filtre kv s'avère très utile lorsque vous voulez parser un champ de type foo=bar comme par exemple une requête HTTP. Ajoutez le filtre kv pour notre example:

{% highlight bash %}
filter {
    [...]
    kv {
      field_split => ","
      source => "msg"
    }
}
{% endhighlight %}
</p>

<p>Relancez logstash et passez lui en entrée la ligne suivante:
{% highlight bash %}08-10-2013 16:33:49.629 [pool-1-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=41,ip=157.55.34.94,brand=Apple,name=iPhone 5C,model=iPhone 5C - Blanc - Disque 16Go,category=Mobile,color=Blanc,options=Disque 16Go,price=599.0{% endhighlight %}
</p>

<p>Vous devez obtenir le résultat suivant:
{% highlight bash %}
{
    "message" => "08-10-2013 16:33:49.629 [pool-1-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=41,ip=157.55.34.94,brand=Apple,name=iPhone 5C,model=iPhone 5C - Blanc - Disque 16Go,category=Mobile,color=Blanc,options=Disque 16Go,price=599.0",
    "@timestamp" => "2013-10-08T14:33:49.629Z",
    "@version" => "1",
    "host" => "lucid64",
    "log_date" => "08-10-2013 16:33:49.629",
    "thread" => "pool-1-thread-1",
    "log_level" => "INFO",
    "classname" => "com.github.vspiewak.loggenerator.SearchRequest",
    "msg" => "id=41,ip=157.55.34.94,brand=Apple,name=iPhone 5C,model=iPhone 5C - Blanc - Disque 16Go,category=Mobile,color=Blanc,options=Disque 16Go,price=599.0",
    "id" => "41",
    "ip" => "157.55.34.94",
    "brand" => "Apple",
    "name" => "iPhone 5C",
    "model" => "iPhone 5C - Blanc - Disque 16Go",
    "category" => "Mobile",
    "color" => "Blanc",
    "options" => "Disque 16Go",
    "price" => "599.0"
}
{% endhighlight %}
Logstash parse maintenant notre ligne de vente et ajoute automatiquement les champs category, brand, name, model, color, options et price.
</p>

<h3>Filtre GeoIP</h3>

<p>Le filtre geoip permet d'ajouter des informations de géolocalisation via une adresse ip (ou hostname).
Logstash utilise la base de donnée GeoCityLite de Maxmind sous license CCA-ShareAlike 3.0. 
<ul><li><a href="https://www.maxmind.com/app/geolite" target="_blank">https://www.maxmind.com/app/geolite</a></li></ul>
Nous allons utiliser une version de GeoCity téléchargé au préalable sur le site Maxmind plutôt que la version embarquée dans Logstash. Copiez le fichier ~/tools/GeoLiteCity.dat dans ~/workshop et rajoutez le filtre dans la configuration logstash:

{% highlight bash %}
filter {
    [...]

    geoip {
        source => "ip"
        database => "./GeoLiteCity.dat"
    }
}
{% endhighlight %}
</p>


<h3>Filtre mutate</h3>

<p>Le filtre mutate est un filtre "couteaux suisses" permettant une multitude de modifications.</p>

<h4>Ajout de tag</h4>

<p>Nous allons ajouter un tag à nos logs afin de différencier les recherches des ventes:
{% highlight bash %}
filter {
    [...]
    if [classname] =~ /.*SellRequest/ {
        mutate { add_tag => "sell" }
    } else if [classname] =~ /SearchRequest$/ {
        mutate { add_tag => "search" }
    }
}
{% endhighlight %}
</p>

<p>Relancez logstash et passez lui en entrée la ligne suivante:
{% highlight bash %}
08-10-2013 16:33:49.629 [pool-1-thread-1] INFO com.github.vspiewak.loggenerator.SearchRequest - id=41,ip=157.55.34.94,brand=Apple,name=iPhone 5C,model=iPhone 5C - Blanc - Disque 16Go,category=Mobile,color=Blanc,options=Disque 16Go,price=599.0
08-10-2013 16:33:49.629 [pool-1-thread-1] INFO com.github.vspiewak.loggenerator.SellRequest - id=41,ip=157.55.34.94,brand=Apple,name=iPhone 5C,model=iPhone 5C - Blanc - Disque 16Go,category=Mobile,color=Blanc,options=Disque 16Go,price=599.0
{% endhighlight %}
</p>

<p>Notez au passage que logstash permet l'utilisation de conditions, pour en savoir plus:
    <ul>
        <li><a target="_blank" href="http://logstash.net/docs/1.2.2/configuration#conditionals">Documentation</a></li>
    </ul>
</p>

<h4>Conversion de type</h4>

<p>Le filtre mutate permet de convertir certains champs en entier, flottant ou string.
Nous ajoutons à notre configuration la conversion des champs id et price:
{% highlight bash %}
filter {
    [...]
    mutate {
        convert => [ "id", "integer" ]
        convert => [ "price", "float" ]
    }   
}
{% endhighlight %}
</p>

<h4>Suppression d'un champ</h4>

<p>Toujours avec le filtre mutate, nous allons supprimer le champ "msg".
Nous avons en effet parsé ce champ avec le filtre kv et n'avons plus besoin de ce doublon d'information.
{% highlight bash %}
filter {
    [...] 
    mutate {
        [...]
        remove_field => [ "msg" ]  
    }  
}
{% endhighlight %}
</p>

<h4>Split d'un champ</h4>

<p>Pour finir avec le filtre mutate, nous allons splité notre champ "options" afin d'avoir un tableau d'options.
{% highlight bash %}
filter {
    [...]
    mutate {
        [...]
       split => [ "options", "|" ]
    }
}
{% endhighlight %}
</p>

<h3>GeoIP et Bettermap</h3>

<p>Le panel Bettermap de Kibana requiert un champ contenant les coordonnées GPS au format Geo_JSON (format: [ longitude, latitude ]).</p>

<p>Ajoutez un champ "geoip.lnglat" contenant le tableau de coordonnées via le "hack" suivant:

{% highlight bash %}
filter {
    [...]
 
    mutate {
        add_field => [ "[geoip][lnglat]", "%{[geoip][longitude]}", "tmplat", "%{[geoip][latitude]}" ]
    }

    mutate {
        merge => [ "[geoip][lnglat]", "tmplat" ]
    }

    mutate {
        convert => [ "[geoip][lnglat]", "float" ]
        remove_field => [ "tmplat" ]
    }
}
{% endhighlight %}
</p>

<h3>Résultat final</h3>

{% highlight bash %}
input {
  stdin { } 
}

filter {
    grok {
        patterns_dir => "./patterns"
        match => ["message","%{LOG_DATE:log_date} \[%{NOTSPACE:thread}\] %{LOGLEVEL:log_level} %{NOTSPACE:classname} - %{GREEDYDATA:msg}"]
    }

    date {
        #timezone => "UTC"
        match => ["log_date","dd-MM-YYYY HH:mm:ss.SSS"]
    }

    kv {
        field_split => ","
        source => "msg"
    }

    geoip {
        source => "ip"
        database => "./GeoLiteCity.dat"
    }

    if [classname] =~ /.*SellRequest/ {
        mutate { add_tag => "sell" }
    } else if [classname] =~ /SearchRequest$/ {
        mutate { add_tag => "search" }
    }

    mutate {
        convert => [ "id", "integer" ]
        convert => [ "price", "float" ]
        remove_field => [ "msg" ]
        split => [ "options", "|" ]
    }

    # hack pour Bettermap panel de Kibana
    mutate {
        add_field => [ "[geoip][lnglat]", "%{[geoip][longitude]}", "tmplat", "%{[geoip][latitude]}" ]
    }

    mutate {
        merge => [ "[geoip][lnglat]", "tmplat" ]
    }

    mutate {
        convert => [ "[geoip][lnglat]", "float" ]
        remove_field => [ "tmplat" ]
    }

}

output {
    stdout { debug => true }
}
{% endhighlight %}

<h2>Lancer la génération de log</h2>

<p>Maintenant que logstash est capable d'analyser nos logs, nous allons lancer notre générateur.</p>

<h3>Création du dossier contenant les futurs logs</h3>
<p>
{% highlight bash %}
$ mkdir /tmp/logstash
{% endhighlight %}
</p>

<h3>Modifier l'entrée de logstash</h3>
<p>
Afin que logstash analyse tous les fichiers de log contenu dans le dossier précédemment créé.
{% highlight bash %}
input {
    file {
        path => "/tmp/logstash/*.log"
    }
}
{% endhighlight %}
</p>

<h3>Lancez le générateur de log</h3>
<p>
    <ul>
        <li>Copiez le jar ~/tools/log-generator.jar dans le dossier ~/workshop</li>
        <li>Lancer le générateur afin de générer une ligne de log chaque seconde dans le fichier /tmp/logstash/workshop.log</li>
    </ul>

{% highlight bash %}
$ java -jar log-generator.jar -n 1 -r 1000 > /tmp/logstash/workshop.log &
{% endhighlight %}
</p>

<h1>ElasticSearch</h1>

<p>Maintenant que Logstash est configuré pour parser nos logs et les transformer dans un format convenable, nous allons stocker ces logs dans ElasticSearch.</p>

<h2>Configuration d'ElasticSearch</h2>

<p>Connectez vous à la vm {{page.vm-es-kibana}} et effectuez les opérations suivantes:
    <ul>
        <li>créez un répertoire workshop</li>
        <li>copiez dans ce répertoire l'archive d'ElasticSearch présente dans le répertoire ~/tools</li>
        <li>éditez le fichier ~/workshop/elasticsearch-0.90.5/config/elasticsearch.yml afin de décommenter la ligne
{% highlight bash %}
cluster.name: elasticsearch
{% endhighlight %}
        </li>
        <li>sauvegardez le fichier</li>
    </ul>
</p>

<h2>Template de mapping ElasticSearch pour les index Logstash</h2>
<p>
    <ul>
        <li>lancez ElasticSearch
{% highlight bash %}
$ ~/workshop/elasticsearch-0.90.5/bin/elasticsearch -f &
{% endhighlight %}
        </li>
        <li>ajoutez le template suivante afin d'utiliser l'analyser "keyword" pour les champs "name", "model", "options" et "email":
{% highlight bash %}
curl -XPUT http://localhost:9200/_template/logstash_per_index -d '{             
    "template" : "logstash*",
                   
    "mappings" : {           
        "_default_" : {
           "_all" : {"enabled" : false},
           "properties" : {
              "@fields" : {
                   "type" : "object",
                   "dynamic": true,  
                   "path": "full"
              },               
              "@message": { "type": "string", "index": "analyzed" },
              "@source": { "type": "string", "index": "not_analyzed" },
              "@source_host": { "type": "string", "index": "not_analyzed" },
              "@source_path": { "type": "string", "index": "not_analyzed" },
              "@tags": { "type": "string", "index": "not_analyzed" },
              "@timestamp": { "type": "date", "index": "not_analyzed" },
              "@type": { "type": "string", "index": "not_analyzed" },
 
              "ip": { "type" : "ip" }, 
              "name": { "type" : "string", "analyzer": "keyword", "index": "analyzed" },
              "model": { "type" : "string", "analyzer": "keyword", "index": "analyzed" },
              "options": { "type" : "string", "analyzer": "keyword", "index": "analyzed" },
              "email": { "type" : "string", "analyzer": "keyword", "index": "analyzed" }
 
            }             
        }
   }
}                
'
{% endhighlight %}
        </li>
    </ul>
</p>

<h2>Branchement de logstash avec ElasticSearch</h2>

<p>ElasticSearch est maintenant configuré. Nous allons donc configurer logstash pour qu'il envoit les logs analysés dans le moteur de recherche.</p>

<p>Modification de la sortie de logstash:
{% highlight bash %}
output {
    elasticsearch {
        embedded => false
        host => "{{page.vm-es-kibana}}"
        cluster => "elasticsearch"
        port => 9300
    }
}
{% endhighlight %}
</p>

<p>C'est tout ce qu'il y a à faire. Vous pouvez maintenant redémarrer logstash et pour vérifier qu'ElasticSearch est bien alimenté, retourner sur la vm d'ElasticSearch et lancez les deux commandes suivantes.</p>

<p>Pour lister les index:
{% highlight bash %}
$ curl -s http://127.0.0.1:9200/_status?pretty=true | grep logstash
{% endhighlight %}
</p>

<p>Pour voir le nombre de documents indexés:
{% highlight bash %}
$ curl -gs -XGET "http://localhost:9200/logstash-*/_count"
{% endhighlight %}
</p>

<h1>Kibana</h1>
